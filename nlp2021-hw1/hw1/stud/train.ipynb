{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "#### Word-In-Context Disambiguation\n",
    "\n",
    "\n",
    "author: Simone Rossetti 499831"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "# general\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from typing import *\n",
    "import numpy as np\n",
    "np.random.seed(41296)\n",
    "import collections\n",
    "import itertools\n",
    "import re\n",
    "from copy import copy\n",
    "from sklearn.decomposition import PCA\n",
    "from random import seed\n",
    "from random import random, randint\n",
    "seed(41296)\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "torch.manual_seed(41296)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "# nltk \n",
    "import nltk\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "source": [
    "The universal tagset consists of the following 12 coarse tags:\n",
    "\n",
    "\n",
    "- VERB - verbs (all tenses and modes)\n",
    "- NOUN - nouns (common and proper)\n",
    "- PRON - pronouns\n",
    "- ADJ - adjectives\n",
    "- ADV - adverbs\n",
    "- ADP - adpositions (prepositions and postpositions)\n",
    "- CONJ - conjunctions\n",
    "- DET - determiners\n",
    "- NUM - cardinal numbers\n",
    "- PRT - particles or other function words\n",
    "- X - other: foreign words, typos, abbreviations\n",
    "- . - punctuation\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip glove embeddings\n",
    "! wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "! gzip -d glove.6B.zip"
   ]
  },
  {
   "source": [
    "# Download and load NLTK APIs used for preprocessing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "# load english stopwords\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "# load english lemmatizer\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
    "# dict mapping from universal POS to nltk wordnet object\n",
    "word_net_type_dict = {'ADJ': nltk.corpus.reader.wordnet.ADJ, \\\n",
    "                    'ADV': nltk.corpus.reader.wordnet.ADV, \\\n",
    "                    'NOUN': nltk.corpus.reader.wordnet.NOUN, \\\n",
    "                    'VERB': nltk.corpus.reader.wordnet.VERB\n",
    "                    }\n",
    "word_net_type_dict = defaultdict(lambda: 'NOUN', word_net_type_dict)\n",
    "# define a simple grammar (extra exercise)\n",
    "grammar = r\"\"\"\n",
    "        NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "        PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "        VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "        CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "        \"\"\"\n",
    "# RegexpParser parse the sentence labeling parts with grammar defined labels\n",
    "chunker = RegexpParser(grammar)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "train_dataframe = pd.read_json('/home/fiorapirri/nlp2021-hw1/data/train.jsonl',lines=True)\n",
    "\n",
    "print(f'train dataframe: {train_dataframe.shape}')\n",
    "\n",
    "dev_dataframe = pd.read_json('/home/fiorapirri/nlp2021-hw1/data/dev.jsonl',lines=True)\n",
    "\n",
    "print(f'dev dataframe: {dev_dataframe.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these tokens are special positions in our embedding tensor\n",
    "RESERVED_TOKENS = {'<pad>': 0, # padding\n",
    "                    '<unk>': 1, # unknown\n",
    "                    '<sep>': 2, # sentences separation\n",
    "                    '<drop>': 3, # dropped word\n",
    "                    '<numb>': 4, # number\n",
    "                    '<punct>': 5} # punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we define NE labels for some further experiments\n",
    "NER_LABELS = ['ORGANIZATION', 'PERSON', 'GSP', 'GPE', \\\n",
    "        'LOCATION', 'FACILITY'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define POS+NE tags wich we will use for some further experiments\n",
    "POS_NE_TAGS = {\n",
    "            'NOUN': 0,\n",
    "            'VERB': 1,\n",
    "            'ADJ': 2,\n",
    "            'ADV': 3,\n",
    "            'GPE': 4,\n",
    "            'ORGANIZATION': 5,\n",
    "            'PERSON': 6,\n",
    "            'LOCATION': 7,\n",
    "            'FACILITY': 8,\n",
    "            'GSP': 9,\n",
    "            '<pad>': 10,\n",
    "            '<drop>': 11,\n",
    "            '<sep>': 12\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path: str, words_limit: int = 100_000) -> dict():\n",
    "    \"\"\"\n",
    "    Load and returns embeddings in a dict from a file path\n",
    "    \"\"\"\n",
    "    word_vectors = dict()\n",
    "    dim = 0\n",
    "    with open(file_path) as f:\n",
    "        for i, line in tqdm(enumerate(f), total=words_limit):\n",
    "            if i == words_limit:\n",
    "                break\n",
    "            # split word (first component) and subsequent values\n",
    "            word, *vector = line.strip().split(' ') \n",
    "            vector = torch.tensor([float(c) for c in vector])\n",
    "            if i == 0:\n",
    "                dim = vector.size(0)\n",
    "            if vector.size(0) == dim:\n",
    "                # fill the dictionary\n",
    "                word_vectors[word] = vector\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_index_and_weights(file_path: str, reserved_tokens: dict = RESERVED_TOKENS,\\\n",
    "    words_limit: int = 100_000) -> (defaultdict, torch.Tensor):\n",
    "    \"\"\"\n",
    "    Returns words index mapping and weights\n",
    "    \"\"\"\n",
    "    word_vectors = load_embeddings(file_path, words_limit)\n",
    "    word_index = dict()\n",
    "    vectors_store = []\n",
    "\n",
    "    for word, vector in word_vectors.items():\n",
    "        word_index[word] = len(vectors_store) + len(RESERVED_TOKENS)\n",
    "        vectors_store.append(vector)\n",
    "  \n",
    "    # compute mean and std of vectors_store in order to\n",
    "    # reduce input noise creating random new vectors for \n",
    "    # reserved tokens\n",
    "    vectors_store = torch.stack(vectors_store)\n",
    "    mean = vectors_store.mean()\n",
    "    std = vectors_store.std()\n",
    "    dim = vectors_store.shape[1]\n",
    "\n",
    "    # assert reserved keys are not in vocabulary yet\n",
    "    for key in RESERVED_TOKENS:\n",
    "        assert key not in word_index    \n",
    "\n",
    "    reserved_tokens = []\n",
    "    # sample new tokens with normal distribution N(mean,std)\n",
    "    for key in RESERVED_TOKENS:\n",
    "        if key == '<drop>':\n",
    "            reserved_tokens.append(torch.zeros((dim,)))\n",
    "        else:\n",
    "            reserved_tokens.append(torch.normal(mean, std, size=(dim,)))\n",
    "        word_index[key] = RESERVED_TOKENS[key]\n",
    "\n",
    "    reserved_tokens = torch.stack(reserved_tokens)\n",
    "    vectors_store = torch.cat([reserved_tokens,vectors_store], dim=0)\n",
    "    # default dict returns 1 (unk token) when unknown word\n",
    "    word_index = defaultdict(lambda: RESERVED_TOKENS['<unk>'], word_index)\n",
    "    \n",
    "    \n",
    "    return word_index, vectors_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we load GloVe word index and embeddings trained on Wikipedia 2014 \n",
    "word_index, vectors_store = load_index_and_weights('glove.6B.300d.txt', words_limit = 400_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similiar words are very near each other, we can use cosine similarity\n",
    "# to visualize this distance in range [0,1]\n",
    "def cosine_similarity(v1: torch.Tensor, v2: torch.Tensor) -> float:\n",
    "    num = torch.sum(v1 * v2)\n",
    "    den = torch.linalg.norm(v1) * torch.linalg.norm(v2)\n",
    "    return (num / den).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words 'united' and 'states' are very near each other\n",
    "cosine_similarity(vectors_store[word_index['united']],vectors_store[word_index['states']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see where some lemma are projected into the plane by PCA\n",
    "# retrieve the trained embeddings\n",
    "embeddings = vectors_store\n",
    "\n",
    "# pick some words to visualise\n",
    "words = list(set(list(train_dataframe['lemma'])))\n",
    "\n",
    "# perform PCA to reduce our 300d embeddings to 2d points that can be plotted\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(embeddings.detach().cpu())\n",
    "\n",
    "indexes = [word_index[x] for x in words[:100]]\n",
    "points = [pca_result[i] for i in indexes]\n",
    "for i,(x,y) in enumerate(points):\n",
    "    plt.plot(x, y, 'ro')\n",
    "    plt.text(x, y, words[i], fontsize=12) # add a point label, shifted wrt to the point\n",
    "plt.title('2D PCA decomposition of embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_chunks(n):\n",
    "    '''\n",
    "    This function extract from a chunked tree the NP, VP and CLAUSEs\n",
    "    '''\n",
    "    chunks = []\n",
    "    if isinstance(n, nltk.tree.Tree):  \n",
    "        if n.label() == 'NP' or n.label() == 'VP' or n.label() == 'CLAUSE':\n",
    "            chunks.extend(n.flatten())\n",
    "        else:\n",
    "            for l in n:\n",
    "                if isinstance(l, nltk.tree.Tree):\n",
    "                    chunks.extend(parse_chunks(l))    \n",
    "    return chunks\n",
    "\n",
    "def get_ne(n):\n",
    "    '''\n",
    "    This function extract NE labels from a tree:\n",
    "        ORGANIZATION \tGeorgia-Pacific Corp., WHO\n",
    "        PERSON \tEddy Bonte, President Obama\n",
    "        LOCATION \tMurray River, Mount Everest\n",
    "        DATE \tJune, 2008-06-29\n",
    "        TIME \ttwo fifty a m, 1:30 p.m.\n",
    "        MONEY \t175 million Canadian Dollars, GBP 10.40\n",
    "        PERCENT \ttwenty pct, 18.75 %\n",
    "        FACILITY \tWashington Monument, Stonehenge\n",
    "        GPE         South East Asia, Midlothian\n",
    "        GSP\n",
    "    '''\n",
    "    ne = []\n",
    "    if isinstance(n, nltk.tree.Tree):\n",
    "        for l in n:\n",
    "            if isinstance(l, nltk.tree.Tree): \n",
    "                if l.label() in NER_LABELS:\n",
    "                    for ll in l.leaves(): \n",
    "                        ne.append((ll[0],l.label()))\n",
    "                else:\n",
    "                    ne.append(l.flatten())\n",
    "            else:\n",
    "                ne.append(l)\n",
    "    return ne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(text: str) -> List[Tuple[str, str]]:\n",
    "    '''\n",
    "    Perform the canonical parsing of a sentence:\n",
    "    1. TOKENIZATION (- STOPWORDS)\n",
    "    2. POS TAGGING (UNIVERSAL POS)\n",
    "    3. LEMMATIZATION\n",
    "    Returns:\n",
    "        - words: processed words\n",
    "        - tags: relative POS tags\n",
    "    '''\n",
    "    parsed = []\n",
    "    text = str(text)\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0–9^,!.\\/’+-=]\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)\\,(\\d+)\", r\"\\g<1>\\g<2>\", text)\n",
    "    text = re.sub(r\"(\\d+)\\.(\\d+)\", r\"\\g<1>\\g<2>\", text)\n",
    "    text = re.sub(r\"(\\d+)\\–(\\d+)\", r\"\\g<1> \\g<2>\", text)\n",
    "    text = re.sub(r\"what’s\", \" what is \", text)\n",
    "    text = re.sub(r\"What’s\", \" What is \", text)\n",
    "    text = re.sub(r\"\\’s\", \" \", text)\n",
    "    text = re.sub(r\"\\’ve\", \" have \", text)\n",
    "    text = re.sub(r\"can’t\", \" can not \", text)\n",
    "    text = re.sub(r\"Can’t\", \" Can not \", text)\n",
    "    text = re.sub(r\"won't\", \" will not \", text)\n",
    "    text = re.sub(r\"Won't\", \" will not \", text)\n",
    "    text = re.sub(r\"n’t\", \" not \", text)\n",
    "    text = re.sub(r\"i’m\", \" i am \", text)\n",
    "    text = re.sub(r\"I’m\", \" I am \", text)\n",
    "    text = re.sub(r\"\\’re\", \" are \", text)\n",
    "    text = re.sub(r\"\\’d\", \" would \", text)\n",
    "    text = re.sub(r\"\\’ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\;\", \" \", text)\n",
    "    text = re.sub(r\"\\,\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"\\!\", \" \", text)\n",
    "    text = re.sub(r\"\\?\", \" \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\°\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" \", text)\n",
    "    text = re.sub(r\"\\+\", \" \", text)\n",
    "    text = re.sub(r\"\\—\", \" \", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \" \", text)\n",
    "    text = re.sub(r\"\\’\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\"\\:\", \" \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" E g \", \" eg \", text)\n",
    "    text = re.sub(r\" U S \", \" american \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\"e mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"(?<=\\d)(st|nd|rd|th)\\b\", \"\", text)\n",
    "    text = re.sub(r\"(\\d+)([A-Za-z]+)\", r\"\\g<1> \\g<2>\", text)\n",
    "    text = re.sub(r\"([A-Za-z]+)(\\d+)\", r\"\\g<1> \\g<2>\", text)\n",
    "    chunked = []\n",
    "    for tokenized in sent_tokenize(text, language='english'):\n",
    "        # Word tokenizers is used to find the words and punctuation in a string\n",
    "        words_list = nltk.word_tokenize(tokenized)\n",
    "        # Using a POS Tagger\n",
    "        words_pos = nltk.pos_tag(words_list)\n",
    "        # words_pos = parse_chunks(chunker.parse(words_pos)) # extra\n",
    "        # label NE entities which do not contribute much to the context\n",
    "        # words_pos = get_ne(nltk.chunk.ne_chunk(words_pos)) # extra\n",
    "        # removing stop words from words_list and lower case all words\n",
    "        words_pos = [(w.lower(),t) for (w,t) in words_pos if (not w.lower() in stop_words)]\n",
    "        words_pos = [(w,nltk.tag.mapping.map_tag('en-ptb', 'universal', t)) if not t in NER_LABELS \\\n",
    "            else (w,t) for (w,t) in words_pos ]\n",
    "        # lemmatize words: was -> be, lemmatizer needs pos tagging\n",
    "        words_pos = [(lemmatizer.lemmatize(w, pos=word_net_type_dict[t]),t) \\\n",
    "          if t in list(word_net_type_dict.keys()) else (w,t) for (w,t) in words_pos]\n",
    "        words_pos = [(w,t) for (w,t) in words_pos if t in POS_NE_TAGS and len(w) > 1]\n",
    "        parsed.extend(words_pos)\n",
    "    return parsed \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see some examples\n",
    "i=100\n",
    "sentence = train_dataframe['sentence1']\n",
    "lemma = train_dataframe['lemma']\n",
    "pos = train_dataframe['pos']\n",
    "start = train_dataframe['start1']\n",
    "end = train_dataframe['end1']\n",
    "\n",
    "words_tags_list = get_words(sentence[i])\n",
    "\n",
    "print(f'SENTENCE:\\n{sentence[i]}\\n')\n",
    "print(f'LEMMA:\\n{lemma[i]}\\n')\n",
    "print(f'POS:\\n{pos[i]}\\n')\n",
    "print(f'PARSING:\\n{words_tags_list}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frequencies(sentences: List[str], embedding_dictionary: Optional[defaultdict] = None, vocab_size: int = 100_000, reserved_tokens: dict = RESERVED_TOKENS):\n",
    "    \"\"\"\n",
    "    Defines the vocabulary to be used. Builds a mapping (word, index) for\n",
    "    each word in the vocabulary.\n",
    "    Returns:\n",
    "        - word2index: dictionary with mapping words-index\n",
    "        - word2freq: dictionary with frequencies for each word\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    for s in sentences:\n",
    "        counter.update([w for (w,t) in get_words(s)])\n",
    "    counter_len = len(counter)\n",
    "    print(\"Number of distinct words: {}\".format(counter_len))\n",
    "    # consider only the (vocab size -1) most common words to build the vocab\n",
    "    dictionary = {key: index+len(RESERVED_TOKENS) for index, (key, _) \\\n",
    "        in enumerate(counter.most_common(vocab_size - 1)) if key not in RESERVED_TOKENS}\n",
    "    for key in RESERVED_TOKENS:\n",
    "        assert key not in dictionary    \n",
    "    for key in RESERVED_TOKENS:\n",
    "        dictionary[key] = RESERVED_TOKENS[key]\n",
    "    word2index = defaultdict(lambda: RESERVED_TOKENS['<unk>'], dictionary)\n",
    "    # dictionary with (word, frequency) pairs -- including only words that are in the vocabulary\n",
    "    frequency = {x: counter[x] for x in dictionary if x not in RESERVED_TOKENS}\n",
    "    word2freq = defaultdict(lambda: 0, frequency)\n",
    "    tot_occurrences = sum(list(frequency.values()))\n",
    "    print(\"Total occurrences of words in dictionary: {}\".format(tot_occurrences))\n",
    "    most_freq_word = max(frequency, key=counter.get)\n",
    "    print(\"Most frequent word in dictionary appears {} times ({})\".format(frequency[most_freq_word],\n",
    "                                                                            most_freq_word))\n",
    "    less_freq_word = min(frequency, key=counter.get)\n",
    "    print(\"Less frequent word in dictionary appears {} times ({})\".format(frequency[less_freq_word],\n",
    "                                                                            less_freq_word))\n",
    "    if embedding_dictionary != None:\n",
    "        unk = len(Counter([w for w in dictionary if embedding_dictionary[w] == RESERVED_TOKENS['<unk>']]))\n",
    "        print(\"Unknown words in embedding model are {}\".format(unk))\n",
    "    return word2index, word2freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's see what is inside the parsed dataset\n",
    "s1 = train_dataframe['sentence1'].copy()\n",
    "s2 = train_dataframe['sentence2'].copy()\n",
    "s1.append(s2, ignore_index=True)\n",
    "word2index, word2freq = count_frequencies(s1, word_index)\n",
    "print('MOST FREQUENT WORDS:\\n')\n",
    "Counter(word2freq).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence: str, lemma: str, pos: str, start: int, end:int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Map a WiC format sample to a the get_words function, \n",
    "    Returns:\n",
    "        - (words, tags): list of processed words in the sentence, POS tagging of the sentence\n",
    "        - index: position of the query lemma\n",
    "    \"\"\"\n",
    "    word = sentence[start:end]\n",
    "    words_tags_list = get_words(sentence[:start]) + [(lemma, pos)] + get_words(sentence[end:]) # process pre word sentence\n",
    "    words = []\n",
    "    tags = []\n",
    "    for word, tag in words_tags_list:\n",
    "        words.append(word)\n",
    "        tags.append(tag)\n",
    "    index = words.index(word)\n",
    "    return words_tags_list, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what preprocess_sentence does?\n",
    "words_tags_list, index = preprocess_sentence(sentence[i], lemma[i], pos[i], start[i], end[i])\n",
    "\n",
    "print(f'SENTENCE:\\n{sentence[i]}\\n')\n",
    "print(f'LEMMA:\\n{lemma[i]}\\n')\n",
    "print(f'POS:\\n{pos[i]}\\n')\n",
    "print(f'PARSING:\\n{words_tags_list}\\n')\n",
    "print(f'QUERY LEMMA INDEX:\\n{index}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_word(word: str, frequency: defaultdict, max_frequency: int) -> bool:\n",
    "    '''\n",
    "    Implements negative frequency sampling and returns true if we can keep the occurrence as training instance.\n",
    "    '''\n",
    "    p_keep = (1 - (frequency[word] / (max_frequency+10e-6)))*0.9+0.1\n",
    "    return np.random.rand() < p_keep # toss a coin and compare it to p_keep to keep the word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_most_frequent(words_tags, index, frequency):\n",
    "    '''\n",
    "    Uses keep_word to decide if keeping words, updates the index position of query lemma\n",
    "    '''\n",
    "    indices = list(range(len(words_tags)))\n",
    "    max_frequency = max(frequency.values())\n",
    "    saved = []\n",
    "    for i in indices:\n",
    "        if i == index:\n",
    "            saved.append(i)\n",
    "        elif keep_word(words_tags[i][0], frequency, max_frequency):\n",
    "            saved.append(i)\n",
    "    pruned_words_tags = [words_tags[i] for i in saved]\n",
    "    index = [w for (w,t) in pruned_words_tags].index(words_tags[index][0])\n",
    "    return pruned_words_tags, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what preprocess_sentence does?\n",
    "words_tags_list, index = preprocess_sentence(sentence[i], lemma[i], pos[i], start[i], end[i])\n",
    "\n",
    "print(f'LEMMA:\\n{lemma[i]}\\n')\n",
    "print(f'SENTENCE:\\n{sentence[i]}\\n')\n",
    "print(f'PARSING:\\n{words_tags_list}\\n')\n",
    "print(f'INDEX:\\n{index}\\n')\n",
    "\n",
    "words_tags_list, index = prune_most_frequent(words_tags_list, index, word2freq)\n",
    "\n",
    "print(f'MOST FREQUENT PRUNING:\\n{words_tags_list}\\n')\n",
    "print(f'INDEX:\\n{index}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_sentence(words_tags: List[Tuple[str,str]], index: int, window: int)-> Tuple[List[Tuple[str,str]], int]:\n",
    "    '''\n",
    "    Cut the sentence at window_size//2 around the query lemma if possible\n",
    "    '''\n",
    "    if len(words_tags) - index > window//2:\n",
    "        first = max(0, index - window//2)\n",
    "        last = min(len(words_tags), window + first)\n",
    "    else:\n",
    "        last = min(len(words_tags), index + window//2)\n",
    "        first = max(0, last - window)\n",
    "    words_tags = words_tags[first:last]\n",
    "    index = index - first\n",
    "    return words_tags, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what window_sentence does?\n",
    "print(f'LEMMA:\\n{lemma[i]}')\n",
    "print(f'SENTENCE:\\n{sentence[i]}')\n",
    "\n",
    "words_tags_list, index = preprocess_sentence(sentence[i], lemma[i], pos[i], start[i], end[i])\n",
    "print(f'WORDS/TAGS:\\n{words_tags_list}')\n",
    "print(f'INDEX:\\n{index}')\n",
    "\n",
    "words_tags_list, index  = window_sentence(words_tags_list, index, 3)\n",
    "print(f'WORDS/TAGS:\\n{words_tags_list}')\n",
    "print(f'INDEX:\\n{index}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2indices(words_tags: List[Tuple[str,str]], dictionary: defaultdict) -> torch.Tensor:\n",
    "    ''' Convert words to words indices '''\n",
    "    return torch.tensor([dictionary[word] for word, tag in words_tags], dtype=torch.long)\n",
    "    \n",
    "def sentence2tagindices(words_tags: List[Tuple[str,str]]) -> torch.Tensor:\n",
    "    ''' Convert tags to tags indices '''\n",
    "    return torch.tensor([POS_NE_TAGS[tag] for word, tag in words_tags], dtype=torch.long)\n",
    "\n",
    "def preprocess_sample(sentence1: str, sentence2: str, lemma: str, pos: str, label: str, \\\n",
    "    start1: int, end1: int, start2: int, end2: int, frequency: defaultdict, \\\n",
    "    dictionary: defaultdict, augment: bool = True) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "    '''\n",
    "    Preprocess a sentence and return a tokenized sequence of word and tags indices\n",
    "    '''\n",
    "    label = torch.tensor([int(label.lower() == 'true')])\n",
    "    words_tags1, index1 = preprocess_sentence(sentence1, lemma, pos, start1, end1)\n",
    "    words_tags2, index2 = preprocess_sentence(sentence2, lemma, pos, start2, end2)\n",
    "    \n",
    "    samples = []\n",
    "    if len(words_tags1)>1 and len(words_tags2)>1:\n",
    "        if augment:\n",
    "            words_tags1, index1 = prune_most_frequent(words_tags1, index1, frequency)  \n",
    "            words_tags2, index2 = prune_most_frequent(words_tags2, index2, frequency)          \n",
    "        \n",
    "        words_tags1, index1  = window_sentence(words_tags1, index1, 20)\n",
    "        words_tags2, index2  = window_sentence(words_tags2, index2, 20)\n",
    "        words1 = sentence2indices(words_tags1, dictionary)\n",
    "        words2 = sentence2indices(words_tags2, dictionary)\n",
    "        indices = torch.tensor([index1, index2])\n",
    "        tags1 = sentence2tagindices(words_tags1)\n",
    "        tags2 = sentence2tagindices(words_tags2)\n",
    "        samples.append((words1, words2, tags1, tags2, indices, label))\n",
    "    return samples  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WiCDataset(Dataset):\n",
    "    '''\n",
    "    This class implement the Word in Context Dataset\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: pd.DataFrame, # table (dataset)\n",
    "        word_index: defaultdict,\n",
    "        vectors_store: Optional[torch.Tensor] = None,\n",
    "        train_set: Optional[bool] = True,\n",
    "        sentence1_column: str = 'sentence1',\n",
    "        sentence2_column: str = 'sentence2',\n",
    "        label_column: str = 'label',\n",
    "        lemma_column: str = 'lemma',\n",
    "        start1_column: str = 'start1',\n",
    "        end1_column: str = 'end1',\n",
    "        start2_column: str = 'start2',\n",
    "        end2_column: str = 'end2',\n",
    "        pos_column: str = 'pos'\n",
    "    ):\n",
    "        self.train_set = train_set\n",
    "        self.sentence1 = data[sentence1_column]\n",
    "        self.sentence2 = data[sentence2_column]\n",
    "        self.label = data[label_column]\n",
    "        self.lemma = data[lemma_column]\n",
    "        self.start1 = data[start1_column]\n",
    "        self.end1 = data[end1_column]\n",
    "        self.start2 = data[start2_column]\n",
    "        self.end2 = data[end2_column]\n",
    "        self.pos = data[pos_column]\n",
    "\n",
    "        if self.train_set and vectors_store != None:\n",
    "            s = data[sentence1_column].copy()\n",
    "            ss = data[sentence2_column].copy()\n",
    "            s.append(ss,ignore_index=True)\n",
    "            _, self.frequency = count_frequencies(s, word_index)\n",
    "            self.dictionary = copy(word_index)\n",
    "            self.vectors = vectors_store.clone().detach()\n",
    "        else:\n",
    "            self.frequency = None\n",
    "            self.dictionary = copy(word_index)\n",
    " \n",
    "        self.samples = self._preprocess_samples()\n",
    "\n",
    "    def _preprocess_samples(self) -> List[Tuple[torch.Tensor,torch.Tensor,torch.Tensor,torch.Tensor]]:\n",
    "        samples = []\n",
    "        for sentence1, sentence2, lemma, pst, label, start1, end1, start2, end2 in \\\n",
    "             zip(self.sentence1, self.sentence2, self.lemma, self.pos, self.label, \\\n",
    "                 self.start1, self.end1, self.start2, self.end2):\n",
    "            sample = preprocess_sample(sentence1, sentence2, lemma, pst, \\\n",
    "                label, start1, end1, start2, end2, self.frequency, self.dictionary, augment = self.train_set)\n",
    "            samples.extend(sample)\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the number of samples in our dataset\n",
    "      return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns the idx-th sample\n",
    "        return self.samples[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's load the train set\n",
    "train_dataset = WiCDataset(train_dataframe, word_index, vectors_store, train_set = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the dev set\n",
    "dev_dataset = WiCDataset(dev_dataframe, word_index, train_set = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how big is the train set?\n",
    "print(len(train_dataset))\n",
    "# what's inside a sample?\n",
    "print(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate function helps us in batching the data\n",
    "def rnn_collate_fn(\n",
    "    data_elements: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    # data_elements is provided in this form (words1, words2, tags1, tags2, indices, label)\n",
    "    X1 = [de[0] for de in data_elements]  # list of index tensors\n",
    "    X2 = [de[1] for de in data_elements]  # list of index tensors\n",
    "\n",
    "    length = [torch.tensor([len(de[0]),len(de[1])]) for de in data_elements]\n",
    "\n",
    "    T1 = [de[2] for de in data_elements]  # list of index tensors\n",
    "    T2 = [de[3] for de in data_elements]  # list of index tensors\n",
    "\n",
    "    # to implement the many-to-one strategy\n",
    "    X1 = torch.nn.utils.rnn.pad_sequence(X1, batch_first=True, padding_value=RESERVED_TOKENS['<pad>']) \n",
    "    X2 = torch.nn.utils.rnn.pad_sequence(X2, batch_first=True, padding_value=RESERVED_TOKENS['<pad>'])\n",
    "    # to implement the many-to-one strategy\n",
    "    T1 = torch.nn.utils.rnn.pad_sequence(T1, batch_first=True, padding_value=POS_NE_TAGS['<pad>']) \n",
    "    T2 = torch.nn.utils.rnn.pad_sequence(T2, batch_first=True, padding_value=POS_NE_TAGS['<pad>'])\n",
    "\n",
    "    batch_size, seq_len1 = X1.shape\n",
    "\n",
    "    # let's create the sep token\n",
    "    sep_token = RESERVED_TOKENS['<sep>']*torch.ones(batch_size, 1).type(torch.LongTensor)\n",
    "    sep_tag_token = POS_NE_TAGS['<sep>']*torch.ones(batch_size, 1).type(torch.LongTensor)\n",
    "\n",
    "    X_index = torch.stack([torch.tensor([de[4][0],de[4][1]]) for de in data_elements])\n",
    "    X_length = torch.stack(length)\n",
    "\n",
    "    y = [de[5] for de in data_elements]\n",
    "    y = torch.stack(y)\n",
    "    \n",
    "    # let's stuck everything\n",
    "    X = torch.cat([X1,sep_token,X2],dim=-1)\n",
    "    T = torch.cat([T1,sep_tag_token,T2],dim=-1)\n",
    "\n",
    "    return X, T, X_index, X_length, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's initialize and visualize what is inside the dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=rnn_collate_fn)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=128, shuffle=False, collate_fn=rnn_collate_fn)\n",
    "\n",
    "for batch in dev_dataloader:\n",
    "    batch_X, batch_T, batch_X_indices, batch_X_len, batch_y = batch\n",
    "    print(batch_X)\n",
    "    print(batch_T)\n",
    "    print(batch_X_indices.shape)\n",
    "    print(batch_X_len)\n",
    "    print(batch_X.shape)\n",
    "    print(batch_y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_penalty(params, l1_lambda=0.001):\n",
    "    \"\"\"Returns the L1 penalty of the params.\"\"\"\n",
    "    l1_norm = sum(p.abs().sum() for p in params)\n",
    "    return l1_lambda*l1_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    '''\n",
    "    Here we define our model\n",
    "    '''\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        n_hidden = 256\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(vectors_store[:400005])\n",
    "        # sequence encoder of size 301: 300 word embedding size + 1 one hot \n",
    "        # encoding of the position of query lemma\n",
    "        self.rnn = torch.nn.GRU(input_size=vectors_store.size(1)+1, hidden_size=n_hidden, \\\n",
    "             num_layers=2, batch_first=True, dropout=0.2, bidirectional=False)\n",
    "        \n",
    "        self.norm1 = torch.nn.BatchNorm1d(n_hidden*2)\n",
    "        self.fc1 = torch.nn.Linear(n_hidden*2, n_hidden)\n",
    "        self.drop1 = nn.Dropout(p=0.2)\n",
    "        self.norm2 = torch.nn.BatchNorm1d(n_hidden)\n",
    "        self.fc2 = torch.nn.Linear(n_hidden, 1)\n",
    "        self.drop2 = nn.Dropout(p=0.2)\n",
    "        self.loss = nn.BCEWithLogitsLoss(reduction = 'mean')\n",
    "        self.noise_mean = 0.0\n",
    "        self.noise_std = vectors_store.std()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        X_indices: torch.Tensor,\n",
    "        X_length: torch.Tensor,\n",
    "        T: Optional[torch.Tensor] = None,\n",
    "        y: Optional[torch.Tensor] = None\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "\n",
    "        batch_size, seq_len = X.shape\n",
    "        # let's find the sep token and divide the sentences\n",
    "        sep_token = torch.where(X[0]==RESERVED_TOKENS['<sep>'])[0]\n",
    "\n",
    "        # here I implemented dropwords\n",
    "        if y is not None and T is not None: \n",
    "            bool_mask = torch.empty(batch_size, seq_len).uniform_(0, 1).to(self.device) > 0.5\n",
    "            mask = torch.ones(batch_size, seq_len).type(torch.LongTensor).to(self.device)\n",
    "            X = torch.where(bool_mask, X, RESERVED_TOKENS['<drop>']*mask)\n",
    "            T = torch.where(bool_mask, T, POS_NE_TAGS['<drop>']*mask)\n",
    "       \n",
    "        # here we can separate the sentences\n",
    "        X1 = X[:,:sep_token]\n",
    "        X2 = X[:,sep_token+1:]\n",
    "        T1 = T[:,:sep_token]\n",
    "        T2 = T[:,sep_token+1:]\n",
    "\n",
    "        pos_tags1 = torch.nn.functional.one_hot(T1, num_classes=len(POS_NE_TAGS)).to(torch.float32)\n",
    "        pos_tags2 = torch.nn.functional.one_hot(T2, num_classes=len(POS_NE_TAGS)).to(torch.float32)\n",
    "\n",
    "        # embedding words from indices\n",
    "        embedding_out1 = self.embedding(X1)\n",
    "        embedding_out2 = self.embedding(X2)\n",
    "\n",
    "        # here I added noise to the input to improve generalization\n",
    "        if y is not None and T is not None:\n",
    "            embedding_out1 +=  torch.normal(self.noise_mean, \\\n",
    "                1.5*self.noise_std, size=embedding_out1.shape).to(self.device)\n",
    "            embedding_out2 +=  torch.normal(self.noise_mean, \\\n",
    "                1.5*self.noise_std, size=embedding_out2.shape).to(self.device)\n",
    "\n",
    "        batch_size, seq_len1, _ = embedding_out1.shape\n",
    "        _, seq_len2, _ = embedding_out2.shape\n",
    "\n",
    "        # here I encode the indices of the query lemma\n",
    "        target1 = torch.nn.functional.one_hot(X_indices[...,0], num_classes=seq_len1)\\\n",
    "            .to(torch.float32).unsqueeze(-1)\n",
    "        target2 = torch.nn.functional.one_hot(X_indices[...,1], num_classes=seq_len2)\\\n",
    "            .to(torch.float32).unsqueeze(-1)\n",
    "\n",
    "        embedding_out1 = torch.cat([embedding_out1, target1],dim=-1)\n",
    "        embedding_out2 = torch.cat([embedding_out2, target2],dim=-1)\n",
    "\n",
    "        # remove padding\n",
    "        embedding_out1 = torch.nn.utils.rnn.pack_padded_sequence(embedding_out1, \\\n",
    "            lengths=X_length[...,0].cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        embedding_out2 = torch.nn.utils.rnn.pack_padded_sequence(embedding_out2, \\\n",
    "            lengths=X_length[...,1].cpu(), batch_first=True, enforce_sorted=False)\n",
    "        # encode the sequence\n",
    "        recurrent_out1, _ = self.rnn(embedding_out1)\n",
    "        recurrent_out2, _ = self.rnn(embedding_out2)\n",
    "        # add padding\n",
    "        recurrent_out1, _ = torch.nn.utils.rnn.pad_packed_sequence(recurrent_out1, \\\n",
    "                batch_first=True, padding_value=POS_NE_TAGS['<pad>'])\n",
    "\n",
    "        recurrent_out2, _ = torch.nn.utils.rnn.pad_packed_sequence(recurrent_out2, \\\n",
    "                batch_first=True, padding_value=POS_NE_TAGS['<pad>'])\n",
    "\n",
    "        # here we utilize the sequences length to retrieve the last token\n",
    "        # output for each sequence\n",
    "        batch_size, seq_len1, hidden_size = recurrent_out1.shape\n",
    "        _, seq_len2, _ = recurrent_out2.shape\n",
    "        # we flatten the recurrent output\n",
    "        # now I have a long sequence of batch x seq_len vectors\n",
    "        flattened_out1 = recurrent_out1.reshape(batch_size * seq_len1, hidden_size)\n",
    "        flattened_out2 = recurrent_out2.reshape(batch_size * seq_len2, hidden_size)\n",
    "        # tensor of the start offsets of each element in the batch\n",
    "        sequences_offsets1 = torch.arange(batch_size, device=self.device) * seq_len1\n",
    "        sequences_offsets2 = torch.arange(batch_size, device=self.device) * seq_len2\n",
    "        # and we use a simple trick to compute a tensor of the indices\n",
    "        # of the last token in each batch element\n",
    "        vect1 = sequences_offsets1 + X_length[...,0]-1\n",
    "        vect2 = sequences_offsets2 + X_length[...,1]-1\n",
    "        \n",
    "        # we retreive the output of the last token\n",
    "        out1 = flattened_out1[vect1]\n",
    "        out2 = flattened_out2[vect2]\n",
    "\n",
    "        # we concatenate the encoded sequences\n",
    "        # and send them to the classifier\n",
    "        out = torch.cat([out1, out2],dim=-1)\n",
    "        out = self.norm1(out)\n",
    "        out = self.drop1(out)\n",
    "        out = self.fc1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.drop2(out)\n",
    "        logits = self.fc2(out)\n",
    "        pred = torch.sigmoid(logits)\n",
    "\n",
    "        result = {}\n",
    "        result['pred'] = pred\n",
    "        result['logits'] = logits\n",
    "\n",
    "        # compute loss\n",
    "        if y is not None:\n",
    "            loss = self.loss(logits, y.to(torch.float32)) \n",
    "            result['loss'] = loss\n",
    "            result['loss_l1'] = loss + l1_penalty(self.fc1.parameters()) \\\n",
    "                 + l1_penalty(self.fc2.parameters())\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net: nn.Module, epoch: int) -> float:\n",
    "    '''\n",
    "    This is a support function to evaluate the model\n",
    "    '''\n",
    "    net.eval()\n",
    "    pred = []\n",
    "    prob = []\n",
    "    label = []\n",
    "    for i, data in enumerate(dev_dataloader):\n",
    "        X, T, X_indices, X_lengths, y = data\n",
    "        output = net(X.to(device), X_indices.to(device), X_lengths.to(device), T.to(device))\n",
    "        prob.extend([float(o) for o in output['logits'].detach().cpu().numpy()])\n",
    "        pred.extend([int(o) for o in output['pred'].detach().round().cpu().numpy()])\n",
    "        label.extend([int(o) for o in y.cpu().numpy()])\n",
    "    loss = net.loss(torch.tensor(prob), torch.tensor(label).to(torch.float32))\n",
    "    print(\"= val loss: %.3f\" % (loss))\n",
    "    log_metric(\"dev loss\", float(loss.numpy()))\n",
    "    score = f1_score(label, pred)\n",
    "    print(\"= f1 score: %.3f\" % (score))\n",
    "    log_metric(\"f1 score\", score)\n",
    "    score = precision_score(label, pred)\n",
    "    print(\"= precision score: %.3f\" % (score))\n",
    "    log_metric(\"dev prec\", score)\n",
    "    score = recall_score(label, pred)\n",
    "    print(\"= recall score: %.3f\" % (score))\n",
    "    log_metric(\"dev rec\", score)\n",
    "    score = accuracy_score(label, pred)\n",
    "    print(\"= accuracy score: %.3f\" % (score))\n",
    "    log_metric(\"dev acc\", score)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlflow import log_metric, log_param, log_artifacts\n",
    "import mlflow\n",
    "mlflow.end_run()\n",
    "mlflow.start_run()\n",
    "from datetime import datetime\n",
    "\n",
    "lr = 0.3\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adadelta(net.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200,300], gamma=0.1)\n",
    "device = torch.device('cuda')\n",
    "net.to(device)\n",
    "net.device = device\n",
    "print(\"Device name: \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "def train(net: nn.Module, optimizer: torch.optim, device: torch.device, epochs: int = 100, \\\n",
    "    accuracy_tolerance: int = 30):\n",
    "    PATH = datetime.now().strftime(\"%m-%d-%Y-%H-%M-%S\")\n",
    "    os.makedirs(PATH)\n",
    "    best_accuracy = 0.0\n",
    "    tol = 0\n",
    "    last_path = None\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        net.train().to(device)\n",
    "        running_loss = 0.0\n",
    "        running_tot_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "            net.train().to(device)\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            X, T, X_indices, X_lengths, y = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(X.to(device), X_indices.to(device), X_lengths.to(device), T.to(device), y.to(device))\n",
    "            loss = outputs['loss_l1']\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), 5)\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            loss = outputs['loss']\n",
    "            running_loss += loss.item()\n",
    "            running_tot_loss += loss.item()\n",
    "            if i % 250 == 249: \n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / 250))\n",
    "                running_loss = 0.0\n",
    "        scheduler.step()\n",
    "        log = ('='*15 + ' epoch: %d '+'='*15) % (epoch + 1)\n",
    "        print(log)\n",
    "        print('= train loss: %.3f' % (running_tot_loss / len(train_dataloader)))\n",
    "        log_metric(\"train loss\", running_tot_loss / len(train_dataloader))\n",
    "        running_tot_loss = 0.0\n",
    "        accuracy = evaluate(net, epoch+1)\n",
    "        # keep only best models\n",
    "        if accuracy > best_accuracy:\n",
    "            tol = 0\n",
    "            best_accuracy = accuracy\n",
    "            path = os.path.join(PATH,'best-%.3f.pt'%(round(best_accuracy,3)))\n",
    "            torch.save({'epoch': epoch,\n",
    "                        'model_state_dict': net.state_dict(),\n",
    "                        'word_index': dict(word_index),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'accuracy': accuracy},\n",
    "                        path)\n",
    "            if last_path != None:\n",
    "                os.remove(last_path)\n",
    "            last_path = path\n",
    "        else:\n",
    "            tol+=1\n",
    "        print('= best accuracy: %.3f' % (best_accuracy))\n",
    "        print('= tolerance: %d' % (tol))\n",
    "        print('='*len(log))\n",
    "        log_metric(\"dev best acc\", best_accuracy)\n",
    "        # if tolerance is exceeded stop\n",
    "        if tol > accuracy_tolerance:\n",
    "            print('Early stop!')\n",
    "            break     \n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "train(net, optimizer, device, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up gpu memory when CUDA memory exceeded is fired\n",
    "import gc\n",
    "gc.collect()\n",
    "net = None\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "device = torch.device('cuda')     # Default CUDA device\n",
    "net.device=device\n",
    "net.load_state_dict(torch.load('./model/best-gru2-parameters-.697.pt')['model_state_dict'])\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net: nn.Module, epoch: int) -> float:\n",
    "    net.eval()\n",
    "    pred = []\n",
    "    prob = []\n",
    "    label = []\n",
    "    for i, data in enumerate(dev_dataloader):\n",
    "        X, T, X_indices, X_lengths, y = data\n",
    "        output = net(X.to(device), X_indices.to(device), X_lengths.to(device), T.to(device))\n",
    "        prob.extend([float(o) for o in output['logits'].detach().cpu().numpy()])\n",
    "        pred.extend([int(o) for o in output['pred'].detach().round().cpu().numpy()])\n",
    "        label.extend([int(o) for o in y.cpu().numpy()])\n",
    "    loss = net.loss(torch.tensor(prob), torch.tensor(label).to(torch.float32))\n",
    "    print(\"= val loss: %.3f\" % (loss))\n",
    "    score = f1_score(label, pred)\n",
    "    print(\"= f1 score: %.3f\" % (score))\n",
    "    score = precision_score(label, pred)\n",
    "    print(\"= precision score: %.3f\" % (score))\n",
    "    score = recall_score(label, pred)\n",
    "    print(\"= recall score: %.3f\" % (score))\n",
    "    score = accuracy_score(label, pred)\n",
    "    print(\"= accuracy score: %.3f\" % (score))\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate(net.to(device), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(net: nn.Module, epoch: int) -> float:\n",
    "    import sklearn\n",
    "    net.cpu().eval()\n",
    "    pred = []\n",
    "    prob = []\n",
    "    label = []\n",
    "    for i, data in enumerate(dev_dataloader):\n",
    "        X1, X2, X_lengths, y = data\n",
    "        output = net(X1, X2, X_lengths)\n",
    "        prob.extend([float(o) for o in output['pred'].detach().numpy()])\n",
    "        pred.extend([int(o) for o in output['pred'].detach().round().numpy()])\n",
    "        label.extend([int(o) for o in y.numpy()])\n",
    "    return sklearn.metrics.confusion_matrix(label,pred,normalize='true')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(net,0.0)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=['True','False'])\n",
    "disp.plot()"
   ]
  }
 ]
}